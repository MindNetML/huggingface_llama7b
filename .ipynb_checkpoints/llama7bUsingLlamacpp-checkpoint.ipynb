{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14def5cf-ce4f-4c44-9cf0-0d6ac54e9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b8c9e-e0dd-4fe7-aa4a-eff616c5d351",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell contains a command that checks the GPU status of the system. The ! at the beginning of the command indicates that this is a shell command being executed from within the notebook. Specifically, nvidia-smi is a command-line utility that provides detailed information about the NVIDIA GPU(s) on the system, including utilization, temperature, memory usage, and more. This command is often used to verify that the system has an NVIDIA GPU and to monitor its status.\n",
    "\n",
    "Let's move on to the content of the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b56969-6a42-49c9-9aaa-b449f535ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is CUDA is installed\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b4f31-9923-4de9-82dc-e36a3721e2e8",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell checks if CUDA (Compute Unified Device Architecture) is installed on the system. CUDA is a parallel computing platform and application programming interface (API) model created by NVIDIA. It allows developers to use NVIDIA GPUs for general-purpose computing.\n",
    "\n",
    "The ! symbol again indicates that a shell command is being executed from within the notebook. The command nvcc --version checks the version of the NVIDIA CUDA Compiler (nvcc). By executing this command, the user can verify that CUDA is installed and determine its version.\n",
    "\n",
    "Let's proceed to the content of the third cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a7c26-d777-4a4d-a702-8fe7ffe71e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update ubuntu packages\n",
    "# This part is optional but in my opinion necessary as not being up to date can cause unexplainable errors later\n",
    "!sudo apt update && sudo apt upgrade -y -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90c8c1-0634-4419-88b0-39a39670e186",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell contains commands to update the Ubuntu packages on the system.\n",
    "\n",
    "The !sudo apt update command retrieves the list of available packages and their versions from the repositories. It essentially updates the package database to know which packages have new versions available for upgrade.\n",
    "The && symbol is a bash operator that allows chaining of commands. The second command will only execute if the first one succeeds.\n",
    "The !sudo apt upgrade -y -y command upgrades the installed packages to their latest versions. The -y flag is used to automatically answer 'yes' to any prompts, allowing the upgrade to proceed without user intervention.\n",
    "The comments in the cell indicate that while this step is optional, it's recommended to ensure that the system is up-to-date, reducing the risk of potential issues caused by outdated packages.\n",
    "\n",
    "Let's examine the content of the fourth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386240fc-bb92-417d-9ae6-0eb81d95b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paperspace already has the 'huggingface_hub' library installed. If using notebook outside of paperspace, install library with ->\n",
    "# -> !pip install huggingface_hub\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "token = \"Delete this text and enter token here inbetween the quotation marks\" # Get Read Token from huggingface account\n",
    "\n",
    "# creates a path called \"Llama-2-7b\" in root directory and places model files inside it. download is about 13GB\n",
    "path = snapshot_download(repo_id=\"meta-llama/Llama-2-7b-chat\",cache_dir=\"Llama-2-7b\", use_auth_token=token) # were downloading the chat version of the 7b model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c43fb-7035-4f9b-8b8d-40126baa8e8d",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell is focused on downloading a model from the Hugging Face Hub. The specific steps and annotations are as follows:\n",
    "\n",
    "The comments at the beginning mention that the huggingface_hub library is already installed in the \"paperspace\" environment. If someone is running this notebook outside of \"paperspace\", they should install the huggingface_hub library using pip.\n",
    "The library huggingface_hub is imported, specifically the snapshot_download function.\n",
    "A variable token is initialized with a placeholder value. This token is intended to be a \"Read Token\" from the user's Hugging Face account. The user is instructed to replace the placeholder text with their actual token.\n",
    "The snapshot_download function is called to download a model. The model being downloaded is the \"chat\" version of the \"Llama-2-7b\" model from the repository \"meta-llama/Llama-2-7b-chat\". The downloaded model will be placed in a directory called \"Llama-2-7b\" in the root directory. The download size is mentioned to be approximately 13GB.\n",
    "Now, let's move on to the content of the fifth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7645aad-1bf5-4148-8016-50f68b955b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we install the github repository for running llama2\n",
    "# huggingface also has libraries to run llama2 but I use the following library for its simplicity and ease of use\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf6e41-e45c-4132-ba2a-bac77d1d30f4",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell is focused on cloning a GitHub repository to the current environment.\n",
    "\n",
    "The initial comments clarify the purpose of this cell. While Hugging Face provides libraries to run the \"llama2\" model, the author of the notebook prefers using a different library from the GitHub repository due to its simplicity.\n",
    "The !git clone command is a shell command executed within the notebook that clones the specified GitHub repository. In this case, the repository \"https://github.com/ggerganov/llama.cpp.git\" contains code for running the \"llama2\" model.\n",
    "Next, we'll examine the content of the sixth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a9e65-9140-4a72-8230-7a2ff54afa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to move from the root directory to the above repository we just downloaded\n",
    "%cd llama.cpp\n",
    "\n",
    "# we have to install the requirements from the repository's 'requirements.txt' file\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca9816-b192-46fc-8635-ba43a945bd33",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell accomplishes two main tasks related to the previously cloned GitHub repository:\n",
    "\n",
    "The %cd magic command is used to change the current working directory of the notebook to the \"llama.cpp\" directory, which was created when the repository was cloned in the previous cell.\n",
    "The !pip install -r requirements.txt command installs the Python packages listed in the \"requirements.txt\" file of the \"llama.cpp\" repository. This ensures that all the necessary dependencies for running the code from that repository are available in the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae2e5d-4499-447d-9183-2730aa67488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We restart the kernel to make use of the libraries we just downloaded\n",
    "# A notice will appear that the kernel died but just ignore it and click okay or whatever pop-up appears and continue to the next block\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688861f-a04a-4f72-8e55-925f7bb119fc",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell forcefully restarts the kernel. Here's a breakdown of its content:\n",
    "\n",
    "The comments explain that the purpose of this cell is to restart the kernel. This is often done to ensure that any newly installed libraries or changes to the environment are recognized and can be used in subsequent cells.\n",
    "The user is informed that they might see a notice about the kernel dying. They are advised to ignore this notice and proceed with the notebook.\n",
    "The os.kill(os.getpid(), 9) command sends a SIGKILL signal to the current process (i.e., the kernel), which causes it to terminate immediately.\n",
    "It's worth noting that when running this cell in an interactive notebook environment, the user may need to manually restart the kernel or continue with the subsequent cells after the kernel has restarted.\n",
    "\n",
    "Moving on, let's inspect the content of the eighth cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2322d-3a9d-4053-8005-1e33f0c67dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to llama.cpp again since we restarted kernel\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0751a54-45ba-4c31-82b0-b2d6928925eb",
   "metadata": {},
   "source": [
    "Description:\n",
    "After restarting the kernel, the current working directory is reset to its default value. This cell changes the working directory back to the \"llama.cpp\" directory (the repository that was cloned earlier). This is accomplished using the %cd magic command.\n",
    "\n",
    "Let's continue with the content of the ninth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d5e23-7bd4-410f-a4df-cc74fc05f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to build the library we just downloaded. if we don't set 'cublas=1' the command will use the cpu instead. ->\n",
    "# -> This will make the models output much slower to generate\n",
    "\n",
    "# This is the reason we also checked at the beginning of the notebook if we had cuda installed. \n",
    "# cuda is necessary to make use of the GPU\n",
    "!make LLAMA_CUBLAS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b5a61-e99e-42ff-b84b-6fa3f9bb6834",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell compiles and builds the \"llama.cpp\" library to make use of GPU acceleration:\n",
    "\n",
    "The comments clarify that the library should be built with GPU support to speed up the model's output generation.\n",
    "The !make LLAMA_CUBLAS=1 command builds the library. The LLAMA_CUBLAS=1 parameter specifies that the CUDA Basic Linear Algebra Subprograms (cuBLAS) library should be used, allowing the model to run on the GPU instead of the CPU. This is why the presence of CUDA was checked earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def5bf3-b994-4529-b0aa-9bbb7dc7e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT ->\n",
    "# The above command built a few files based on the machine we are using. If you start a new paperspace instance with a different GPU, ->\n",
    "# this guide will no longer work. A fix is to first run '!make clean' followed by '!make LLAMA_CUBLAS=1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a1efd-8619-4a11-ae9d-60cb960aa865",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell provides important information and guidance regarding the previously executed build command:\n",
    "\n",
    "The comments indicate that the build command from the previous cell generated files specific to the current machine and GPU setup.\n",
    "If a user starts a new instance with a different GPU configuration, the guide may not work as expected.\n",
    "A solution is provided: If users encounter issues on a different setup, they should first run !make clean to clean up the generated files and then re-run the !make LLAMA_CUBLAS=1 command to rebuild the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb39808-47dc-4d9b-82f5-5c79b184e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now going to convert the model as we cannot use it in the format we downloaded it in. Usually, the format that its in is a .chk file format\n",
    "# The repository we are using is converting it into a .gguf format.\n",
    "# you can check the file format of the model we downloaded uncommenting the following code:\n",
    "!ls ../Llama-2-7b/models--meta-llama--Llama-2-7b-chat/snapshots/2abbae1937452ebd4eecb63113a87feacd6f13ac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4f020-20e8-4e95-8b08-38fb08d1cfd3",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell provides information about the model's format and lists the files in the model's directory:\n",
    "\n",
    "The comments explain that the model, as downloaded, is typically in the .chk file format, but the \"llama.cpp\" repository will be used to convert it into the .gguf format for compatibility.\n",
    "The !ls command lists the files in the specified directory, which is where the model was downloaded. This allows the user to inspect the model's current file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5821d7-7a50-4960-bccf-499f0361e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert .chk file to .gguf\n",
    "!python3 convert.py ../Llama-2-7b/models--meta-llama--Llama-2-7b-chat/snapshots/2abbae1937452ebd4eecb63113a87feacd6f13ac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f104078-6382-49d8-ab93-5c53f4938904",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell converts the model from the .chk file format to the .gguf format:\n",
    "\n",
    "The convert.py script from the \"llama.cpp\" repository is invoked to perform the conversion.\n",
    "The path provided as an argument to the convert.py script points to the location of the model in the .chk format. The script will then convert this model to the desired .gguf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231bc21-372d-4634-ab17-b14b8f592bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code made a new file called 'ggml-model-f16.gguf' this is the converted llama2 model in .gguf format\n",
    "\n",
    "# now we need to quantize the model. This part is necessary if we are using a weaker machine. \n",
    "# we can technically run it as is but it'll genrate text alot slower than if it were quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef80923-fbf3-4c26-a3df-1a6d64cdaffc",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell provides explanatory comments about the next steps in the process:\n",
    "\n",
    "The first comment informs the user that the previous cell created a new file named ggml-model-f16.gguf, which is the converted \"llama2\" model in the .gguf format.\n",
    "The subsequent comments explain the need for model quantization. Quantizing the model can make it run more efficiently, especially on weaker hardware. While the model can run without quantization, the generation of text would be slower compared to a quantized model.\n",
    "Let's inspect the content of the fourteenth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e2a4e-9695-464a-881a-91c61e4af243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get a test run with the un-quantized model with this code block, \n",
    "# it'll generate a few random words so you can get a sense of its speed along with other model settings\n",
    "!./main -m ../Llama-2-7b/models--meta-llama--Llama-2-7b-chat/snapshots/2abbae1937452ebd4eecb63113a87feacd6f13ac/ggml-model-f16.gguf -n 128\n",
    "\n",
    "# As you can see, it's really slow to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24323db-2b58-46d2-8600-01fda30d15c1",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell tests the speed of the un-quantized model:\n",
    "\n",
    "The initial comments explain that this code block allows the user to test the speed of the un-quantized model by generating a few random words.\n",
    "The !./main command runs the main executable of the \"llama.cpp\" repository with the specified model (ggml-model-f16.gguf) and a setting to generate 128 tokens (-n 128).\n",
    "The concluding comment notes that the un-quantized model is slow in generating text, emphasizing the potential benefits of quantization.\n",
    "Let's proceed to the content of the fifteenth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591bcc9a-3717-4fb4-a2c7-0fa42f00771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's move the model so we can quantize it and afterwards chat with it\n",
    "!mv ../Llama-2-7b/models--meta-llama--Llama-2-7b-chat/snapshots/2abbae1937452ebd4eecb63113a87feacd6f13ac/ggml-model-f16.gguf models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312c805-91d4-45a5-af94-f6993b66e123",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell moves the model file to a different directory in preparation for the quantization and subsequent usage:\n",
    "\n",
    "The comment provides context, indicating that the model will be moved to a new location for the next steps: quantization and chatting.\n",
    "The !mv command is a shell command that moves the specified model file (ggml-model-f16.gguf) to the \"models\" directory.\n",
    "Now, let's examine the content of the sixteenth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd71b5f-9263-4645-90cf-a9138545df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize the model\n",
    "# the following code seems to be bugged. its probably since its meant for the terminal not for a notebook. Itll run but it wont output the quantized model the directory\n",
    "# copy and paste the below code into the terminal. Make sure your current directory is inside llama.cpp folder before running\n",
    "!./quantize models/ggml-model-f16.gguf ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3dc65-588b-49f2-8270-bd7e458b241c",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell attempts to quantize the model:\n",
    "\n",
    "The initial comments explain that there might be an issue with the quantization command when executed from the notebook, as it might have been designed to run in a terminal environment. Although the command will execute, it might not produce the expected output.\n",
    "The user is advised to copy the quantization command and execute it in a terminal, ensuring they are inside the \"llama.cpp\" directory.\n",
    "The !./quantize command is intended to quantize the ggml-model-f16.gguf model and produce a new quantized model named ggml-model-q4_0.gguf with the quantization scheme q4_0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad2e66-cd5b-4670-9565-61e3d7f23fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linux was giving me some issues (like always) so to fix them I saved the model to the llamma.cpp root directory. lets move the quantized model to the models directory\n",
    "!mv ggml-model-q4_0.gguf models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a46ee6-a9ee-4527-976b-2d9a8acbfeb6",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell is focused on moving the quantized model to the appropriate directory:\n",
    "\n",
    "The comment mentions that due to issues encountered in Linux, the model was saved in the root directory of \"llama.cpp\".\n",
    "The !mv command then moves the quantized model, ggml-model-q4_0.gguf, from the root directory of \"llama.cpp\" to the \"models\" subdirectory.\n",
    "Now, let's review the content of the eighteenth cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd00de-59b9-4832-b622-d94cd33de903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that the model is reformmatted and quantized, we can finally start using it\n",
    "# a premade chat prompt is already included in the llama.cpp repositroy so we'll use it\n",
    "# the examples require us to place the model inside the models folder (like we already have) but it also requires it inside another folder called 'llama-7b'. \n",
    "# we'll create the folder, move the model inside it and run the model with example\n",
    "\n",
    "!mkdir models/llama-7b\n",
    "!mv models/ggml-model-q4_0.gguf models/llama-7b/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fa7f4-0279-47a0-95b5-9bcf5a79e990",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell prepares the environment to use the quantized model by moving it to a specific directory structure required by the examples:\n",
    "\n",
    "The initial comments explain that now that the model is reformatted and quantized, it's ready for use. The \"llama.cpp\" repository includes a premade chat prompt that the notebook intends to utilize.\n",
    "The chat examples in the \"llama.cpp\" repository expect the model to be placed inside a folder named \"llama-7b\" within the \"models\" directory.\n",
    "The !mkdir command creates the \"llama-7b\" folder inside the \"models\" directory.\n",
    "The !mv command moves the quantized model ggml-model-q4_0.gguf into the newly created \"llama-7b\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a01cc-f72d-4dac-ba49-6d755a2869f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just copy the below code and paste it into the terminal to chat with llama2-7b\n",
    "./examples/chat.sh\n",
    "\n",
    "\n",
    "# the example named the model bob through a prompt, but you can use other code to run your own prompt for example: \n",
    "./main -m models/llama-7b/ggml-model-q4_0.gguf -n -1 --color -r \"User:\" --in-prefix \" \" -i -p \\\n",
    "'User: Hi\n",
    "AI: Hello. I am an AI chatbot. Would you like to talk?\n",
    "User: Sure!\n",
    "AI: What would you like to talk about?\n",
    "User:'\n",
    "\n",
    "#  copy and paste the above code in the terminal, make sure your in the llama.cpp folder when running\n",
    "\n",
    "# Unfortunatly, you can't run the model in the notebook. if you try running it, it'll run and stay on but since the model requires input and this notebook doesn't have a way to\n",
    "# send input to the terminal while it runs, its not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1265c-e7e1-459d-a303-63f3f829a148",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell provides commands to chat with the \"llama2-7b\" model:\n",
    "\n",
    "The initial comment instructs users to copy the ./examples/chat.sh command and execute it in a terminal to start a chat session with the model.\n",
    "A note mentions that the provided example names the model as \"bob\" in the chat prompt. However, if users want to customize the chat prompt, they can use an alternative command that specifies a different conversation format.\n",
    "The ./main command provided is a more detailed way to start a chat session, specifying model parameters and an initial chat prompt. The given conversation format sets up an initial greeting between the user and the AI model.\n",
    "A cautionary note clarifies that the model cannot be run directly in the notebook. This is because the model requires real-time user input, which can't be provided within the notebook's interface. Thus, users are encouraged to run the model in a terminal.\n",
    "Let's move on to the content of the twentieth (and final) cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbf59d-c44f-47f0-952d-c7a3e9fa2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run the model in the terminal and then run this block to see how gpu usage running the model takes. \n",
    "# acording to the info, it takes about 370MB of the GPU to run the model\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd467d65-a6d6-408c-ac1d-b00b4b38f4ca",
   "metadata": {},
   "source": [
    "Description:\n",
    "This cell allows users to monitor GPU usage while the model is running:\n",
    "\n",
    "The comments suggest that after running the model in a terminal, users can execute this cell to observe the GPU resource utilization.\n",
    "The provided information mentions that, based on previous observations, the model takes up about 370MB of the GPU memory when running.\n",
    "The !nvidia-smi command, which we've seen earlier in the notebook, provides detailed statistics about the GPU's status, including memory usage, temperature, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
